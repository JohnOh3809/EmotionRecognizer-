{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rPMhWLSY691"
   },
   "outputs": [],
   "source": [
    "!pip -q install mediapipe opencv-python-headless tqdm scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz_SFirGbMmW"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, glob, shutil, json, random, math\n",
    "from pathlib import Path\n",
    "\n",
    "# === CHANGE THIS to where you uploaded CAER ===\n",
    "DRIVE_CAER_ZIP_OR_FOLDER = \"/content/drive/MyDrive/datasets/CAER\"\n",
    "\n",
    "# Working directory in Colab VM\n",
    "WORKDIR = \"/content/caer_work\"\n",
    "DATA_ROOT = os.path.join(WORKDIR, \"CAER\")  # where CAER will be extracted\n",
    "\n",
    "os.makedirs(WORKDIR, exist_ok=True)\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "\n",
    "print(\"DRIVE_CAER_ZIP_OR_FOLDER =\", DRIVE_CAER_ZIP_OR_FOLDER)\n",
    "print(\"DATA_ROOT               =\", DATA_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRcIvfKPbaH2"
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "\n",
    "def unzip_if_needed(src_dir, dst_dir):\n",
    "    # Case A: already-extracted CAER folder exists in Drive\n",
    "    possible = [\n",
    "        os.path.join(src_dir, \"CAER\"),\n",
    "        os.path.join(src_dir, \"caer\"),\n",
    "    ]\n",
    "    for p in possible:\n",
    "        if os.path.isdir(p):\n",
    "            if os.path.exists(dst_dir) and len(os.listdir(dst_dir)) > 0:\n",
    "                print(\"DATA_ROOT already has contents; skipping copy.\")\n",
    "                return\n",
    "            print(f\"Copying extracted folder from {p} → {dst_dir}\")\n",
    "            !rsync -a \"{p}/\" \"{dst_dir}/\"\n",
    "            return\n",
    "\n",
    "    # Case B: single zip\n",
    "    zips = glob.glob(os.path.join(src_dir, \"*.zip\"))\n",
    "    if len(zips) == 1:\n",
    "        z = zips[0]\n",
    "        print(\"Unzipping:\", z)\n",
    "        !unzip -q \"{z}\" -d \"{dst_dir}\"\n",
    "        return\n",
    "\n",
    "    # Case C: multiple zip parts (example patterns)\n",
    "    # If your download is split: caer_split.zip, caer_split.z01, ...\n",
    "    parts = sorted(glob.glob(os.path.join(src_dir, \"*.z*\")))\n",
    "    if parts:\n",
    "        print(\"Found possible split-zip parts:\", parts[:5], \"...\")\n",
    "        print(\"If this is a split zip, you can recombine. Example:\")\n",
    "        print('  !zip -s 0 \"caer_split.zip\" --out caer.zip')\n",
    "        print('  !unzip caer.zip -d \"{dst_dir}\"')\n",
    "        return\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find an extracted CAER folder or a zip in DRIVE_CAER_ZIP_OR_FOLDER. \"\n",
    "        \"Upload CAER into that directory first.\"\n",
    "    )\n",
    "\n",
    "unzip_if_needed(DRIVE_CAER_ZIP_OR_FOLDER, DATA_ROOT)\n",
    "\n",
    "# Print a quick tree preview\n",
    "for root, dirs, files in os.walk(DATA_ROOT):\n",
    "    print(root, \"dirs:\", dirs[:5], \"files:\", files[:5])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rOtN_6Qbcyn"
   },
   "outputs": [],
   "source": [
    "VIDEO_EXTS = {\".mp4\", \".avi\", \".mov\", \".mkv\"}\n",
    "\n",
    "# CAER's 7 categories (site lists 7 categories; these are the standard set)\n",
    "# If your folder names differ (e.g., lowercase), we normalize.\n",
    "CLASSES = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "class_to_idx = {c:i for i,c in enumerate(CLASSES)}\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    mapping = {\n",
    "        \"anger\": \"angry\",\n",
    "        \"angry\": \"angry\",\n",
    "        \"disgust\": \"disgust\",\n",
    "        \"fear\": \"fear\",\n",
    "        \"happy\": \"happy\",\n",
    "        \"neutral\": \"neutral\",\n",
    "        \"sad\": \"sad\",\n",
    "        \"surprise\": \"surprise\",\n",
    "    }\n",
    "    return mapping.get(s, s)\n",
    "\n",
    "def infer_split_from_path(p: str) -> str:\n",
    "    parts = [x.lower() for x in Path(p).parts]\n",
    "    if \"train\" in parts: return \"train\"\n",
    "    if \"validation\" in parts or \"val\" in parts: return \"val\"\n",
    "    if \"test\" in parts: return \"test\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def infer_label_from_path(p: str) -> str:\n",
    "    # Look for a directory name matching a class\n",
    "    parts = [normalize_label(x) for x in Path(p).parts]\n",
    "    for x in parts[::-1]:\n",
    "        if x in class_to_idx:\n",
    "            return x\n",
    "    return \"unknown\"\n",
    "\n",
    "def collect_videos(data_root: str):\n",
    "    all_files = []\n",
    "    for ext in VIDEO_EXTS:\n",
    "        all_files.extend(glob.glob(os.path.join(data_root, \"**\", f\"*{ext}\"), recursive=True))\n",
    "\n",
    "    items = []\n",
    "    for f in all_files:\n",
    "        split = infer_split_from_path(f)\n",
    "        label = infer_label_from_path(f)\n",
    "        if split == \"unknown\" or label == \"unknown\":\n",
    "            continue\n",
    "        items.append((f, split, label, class_to_idx[label]))\n",
    "    return items\n",
    "\n",
    "items = collect_videos(DATA_ROOT)\n",
    "print(\"Found labeled videos:\", len(items))\n",
    "print(\"Example:\", items[0] if items else None)\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Split counts:\", Counter([s for _,s,_,_ in items]))\n",
    "print(\"Label counts:\", Counter([lab for _,_,lab,_ in items]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kcvF_a3bedm"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mediapipe as mp\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "CACHE_DIR = os.path.join(WORKDIR, \"pose_cache\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Sequence length and sampling\n",
    "SEQ_LEN = 32\n",
    "TARGET_FPS = 10  # sample frames at ~10fps (approx)\n",
    "MIN_VIS = 0.3    # landmark visibility threshold\n",
    "\n",
    "def make_cache_path(video_path: str) -> str:\n",
    "    # stable filename based on path hash\n",
    "    import hashlib\n",
    "    h = hashlib.md5(video_path.encode(\"utf-8\")).hexdigest()\n",
    "    return os.path.join(CACHE_DIR, f\"{h}_T{SEQ_LEN}.npy\")\n",
    "\n",
    "def sample_frame_indices(total_frames: int, fps: float, seq_len: int, target_fps: int, train: bool):\n",
    "    if total_frames <= 0:\n",
    "        return np.linspace(0, 0, seq_len).astype(int)\n",
    "\n",
    "    stride = max(int(round(fps / max(target_fps, 1))), 1)\n",
    "    needed = seq_len * stride\n",
    "\n",
    "    if train and total_frames >= needed:\n",
    "        start = random.randint(0, max(total_frames - needed, 0))\n",
    "        idx = start + np.arange(seq_len) * stride\n",
    "    else:\n",
    "        # uniform coverage\n",
    "        idx = np.linspace(0, total_frames - 1, seq_len).astype(int)\n",
    "\n",
    "    idx = np.clip(idx, 0, max(total_frames - 1, 0))\n",
    "    return idx.astype(int)\n",
    "\n",
    "def extract_pose_sequence(video_path: str, seq_len=SEQ_LEN, target_fps=TARGET_FPS, train=False):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Could not open video: {video_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps is None or fps <= 1e-3:\n",
    "        fps = 25.0\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_idxs = sample_frame_indices(total_frames, fps, seq_len, target_fps, train=train)\n",
    "\n",
    "    # MediaPipe Pose (single-person). For crowded scenes, you may want a person-detector + crop.\n",
    "    pose = mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        enable_segmentation=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "\n",
    "    seq = np.zeros((seq_len, 33, 3), dtype=np.float32)  # (T, K, [x,y,vis])\n",
    "    ok_count = 0\n",
    "\n",
    "    for t, fi in enumerate(frame_idxs):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(fi))\n",
    "        ok, frame = cap.read()\n",
    "        if not ok or frame is None:\n",
    "            continue\n",
    "\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        res = pose.process(rgb)\n",
    "\n",
    "        if res.pose_landmarks is None:\n",
    "            continue\n",
    "\n",
    "        lms = res.pose_landmarks.landmark\n",
    "        for k in range(33):\n",
    "            seq[t, k, 0] = lms[k].x\n",
    "            seq[t, k, 1] = lms[k].y\n",
    "            seq[t, k, 2] = lms[k].visibility\n",
    "        ok_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    pose.close()\n",
    "\n",
    "    return seq, ok_count\n",
    "\n",
    "def normalize_skeleton(seq):\n",
    "    \"\"\"\n",
    "    Normalize to reduce camera effects:\n",
    "    - center at mid-hip\n",
    "    - scale by shoulder-hip distance (rough body scale)\n",
    "    \"\"\"\n",
    "    seq = seq.copy()\n",
    "    # indices: left_hip=23, right_hip=24, left_shoulder=11, right_shoulder=12 (MediaPipe Pose)\n",
    "    hip = (seq[:, 23, :2] + seq[:, 24, :2]) / 2.0\n",
    "    sh  = (seq[:, 11, :2] + seq[:, 12, :2]) / 2.0\n",
    "    scale = np.linalg.norm(sh - hip, axis=1, keepdims=True)  # (T,1)\n",
    "    scale = np.maximum(scale, 1e-3)\n",
    "\n",
    "    seq[:, :, 0] = (seq[:, :, 0] - hip[:, 0:1]) / scale\n",
    "    seq[:, :, 1] = (seq[:, :, 1] - hip[:, 1:2]) / scale\n",
    "    # keep visibility as-is\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-Bi9iY_bgD2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CAERSkeletonDataset(Dataset):\n",
    "    def __init__(self, items, split, cache_dir=CACHE_DIR):\n",
    "        self.rows = [(vp, y) for (vp, sp, _, y) in items if sp == split]\n",
    "        self.split = split\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, y = self.rows[idx]\n",
    "        cache_path = make_cache_path(video_path)\n",
    "\n",
    "        if os.path.exists(cache_path):\n",
    "            seq = np.load(cache_path)  # (T,33,3)\n",
    "        else:\n",
    "            seq_raw, ok_count = extract_pose_sequence(video_path, train=(self.split==\"train\"))\n",
    "            seq = normalize_skeleton(seq_raw)\n",
    "            np.save(cache_path, seq)\n",
    "\n",
    "        # flatten keypoints\n",
    "        x = seq.reshape(seq.shape[0], -1)  # (T, 33*3)\n",
    "\n",
    "        # Optional: mask out low-visibility points\n",
    "        # (keeps x stable; simple baseline)\n",
    "        # vis = seq[:,:,2]  # (T,33)\n",
    "        # You could incorporate vis into features if desired.\n",
    "\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "train_ds = CAERSkeletonDataset(items, \"train\")\n",
    "val_ds   = CAERSkeletonDataset(items, \"val\")\n",
    "test_ds  = CAERSkeletonDataset(items, \"test\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(len(train_ds), len(val_ds), len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_O5b_yaSbhn_"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTMSkeletonEmotion(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=256, num_layers=2, num_classes=7, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, hidden)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden * 2, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,D)\n",
    "        x = self.proj(x)\n",
    "        out, _ = self.lstm(x)      # (B,T,2H)\n",
    "        feat = out[:, -1, :]       # last timestep\n",
    "        return self.head(feat)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTMSkeletonEmotion(in_dim=33*3, num_classes=len(CLASSES)).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frjt0onVbirv"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def run_epoch(model, loader, train: bool):\n",
    "    model.train(train)\n",
    "    all_preds, all_y = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for x, y in tqdm(loader, leave=False):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.detach().cpu().tolist())\n",
    "        all_y.extend(y.detach().cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    acc = accuracy_score(all_y, all_preds) if len(all_y) else 0.0\n",
    "    return avg_loss, acc\n",
    "\n",
    "best_val = -1.0\n",
    "best_path = os.path.join(WORKDIR, \"best_skeleton_model.pt\")\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = run_epoch(model, train_loader, train=True)\n",
    "    va_loss, va_acc = run_epoch(model, val_loader, train=False)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
    "\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        torch.save({\"model\": model.state_dict(), \"classes\": CLASSES}, best_path)\n",
    "        print(\"Saved best →\", best_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZWNamaibj7D"
   },
   "outputs": [],
   "source": [
    "ckpt = torch.load(best_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "te_loss, te_acc = run_epoch(model, test_loader, train=False)\n",
    "print(f\"TEST | loss {te_loss:.4f} acc {te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4iMuAvribmaM"
   },
   "outputs": [],
   "source": [
    "def predict_video_emotion(video_path: str):\n",
    "    model.eval()\n",
    "    cache_path = make_cache_path(video_path)\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        seq = np.load(cache_path)\n",
    "    else:\n",
    "        seq_raw, _ = extract_pose_sequence(video_path, train=False)\n",
    "        seq = normalize_skeleton(seq_raw)\n",
    "\n",
    "    x = torch.tensor(seq.reshape(seq.shape[0], -1), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        pred = int(torch.argmax(logits, dim=1).item())\n",
    "    return CLASSES[pred]\n",
    "\n",
    "# Example usage:\n",
    "# some_video = train_ds.rows[0][0]\n",
    "# print(some_video, \"→\", predict_video_emotion(some_video))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
